---
title: "Practical Machine Learning Project"
author: "Kevin Spring"
date: "09/18/2014"
output: html_document
---

### Background

Wearable technology combine computer technology with clothing or accessories such as watches, wristbands, or waistbands. Some of these devies are capable of measuring users movement, respiration, heart beat, and many other physiological functions. The idea is that with more quantifiable data about the body the user can live a healtier life.

We have been provided a data set from 6 participants doing barbell lifts correctly and incorrectly in 5 different ways(classe variable: A, B, C, D, E). The data is from accelerometers located on the belt, forearm, arm, and dumbell. **The purpose of this work is to determine if it is possible to accurately predict which way the participant is conducting their exercise from the variables generated by the wearable accelerometers**.

### Load data
```{r}

## Libraries needed in this work
library(randomForest) # to build the model
library(caret) # for k-fold CV
library(irr) # needed to measure kappa

testing <- read.csv("data/pml-testing.csv")
training <- read.csv("data/pml-training.csv")
dim(training)
```

### Data cleanup

The data contains 160 columns and 19622 rows. Not all of the variables are useful in prediction. For example there is date-time variables and a variable that identifies the participant in the experiment. Also, when examining the data there are many variables that only have 19216 or more missing values. Using the below code these variables are removed and leaves the data set with 52 possible predictors.

```{r}
# Remove any nonquantitative variables
trainingSub <- training[-c(1:7)]
trainingSubQuant <- trainingSub[, sapply(trainingSub, is.numeric)]

## Add back the Classe variable
trainingSubQuant$classe <- training$classe

# Data cleanup

## Remove all columsn that are only NA values
# trainingSubQuant <- trainingSubQuant[ , !apply( trainingSubQuant , 2 , function(x) all(is.na(x)) ) ]
trainingSubQuant <- trainingSubQuant[,! colSums(is.na(trainingSubQuant)) >= 19216]

dim(trainingSubQuant)
```

### K-fold Cross Validation
Models must be accurate to be useful. Thus, we want to estimate the performance of our model. K-fold cross validation (k-fold CV) divides the data into *k* separate random folds. In this study when a random fold is divided 90% will be kept for the training data set while 10% will be a testing data set. This is repeated on the main training data set a *k* number of times to create *k* number of new training and *k* number of new testing data sets. 

Kappa is a statistical measure that adjusts the accuracy measurement of the model by taking into account the possibility of a correct prediction by chance alone. Kappa values range from 0 (poor agreement) to 1 (very good agreement). For each of the separate random division of training dataset each k-fold division divides the data set into 90% training and 10% testing. Then the accuracy of the prediction is measured from a Random Forest algorithm. Kappa is found with the following equation:

$$
k = \frac{Pr(a) - Pr(e)}{1 - Pr(e)}
$$

where $Pr(a)$ is the proportion of the actual agreement between the model and the true values and $Pr(e)$ is the proportion of expected agreement between the model and the true values.

This is repeated for each k-fold and the kappa measurements are averaged over the *k* number of folds.  

```{r}
folds <- createFolds(y=trainingSubQuant$classe, # Create 10 random folds
                       k = 10,
                       list=T,
                       returnTrain=T)

# The following function divides the trainingSubQuant data set into training and test data, creates a decision tree using the C5.0() function on the training data, generates a set of predictions from the test data, and compares the predicted and actual values using the kappa2() function.

cv_results <- lapply(folds, function(x) {
  train <- trainingSubQuant[x, ]
  test <- trainingSubQuant[-x, ]
  model <- randomForest(classe ~ ., data=train, ntree=50)
  pred <- predict(model, test)
  actual <- test$classe
  kappa <- kappa2(data.frame(actual, pred))$value
  return(kappa)
})
```

In this study 10 random folds are taken from the data set. The out of sample error as measured with kappa was found to be **`r mean(unlist(cv_results))`** for the k-fold CV. Since it is so close to the value 1, this kappa indicates a very good agreement.

### Building the model

Random forests is used to generate the prediction model from the training data set. Random forests is a decision tree based approach that uses a combination of decision trees to make predictions. The model is built on the entire training set data on 50 decision trees. This algorithm was chosen as Random Forests will automatically measure for the most important predictors and can be used for data sets with a large number of features.

```{r}
trainingSet <- trainingSubQuant

# Random forest
m <- randomForest(classe ~ ., data=trainingSet, ntree=50)
```

### Predicting the routine

The testing data set consits of 20 rows and 160 variables. The question is to know if we can accurately predict what exercise routine (variable classe) is being accomplished by a set of quantitative measurements. The testing data set does not contain the classe variable but will be predicted from the quantitative variables present in the data set.

The same variables used in the model generated from the training set must also be used to predict the exercise routine in the testing data sets. This means we must follow the same cleanup workflow on the testing data set.

```{r}
# Remove any nonquantitative variables
testingSub <- testing[-c(1:7)]
testingSubQuant <- testingSub[, sapply(testingSub, is.numeric)]

## Add back the Classe variable
testingSubQuant$classe <- testing$classe

# Data cleanup

## Remove all columsn that are only NA values
testingSubQuant <- testingSubQuant[,! colSums(is.na(testingSubQuant)) >= 19216]

pred <- as.character(predict(m, testingSubQuant))
```

### Conclusions
The training data is predicted to be the following routines (in order): `r pred`. Using k-fold CV along with Random Forests I am highly confident that that the model accurately predicts the exercise routine. From this data set it is possible to accruately predict the 5 different ways in which the study participants were performing the dumbell exercise routine. This information could be expanded to other exercise routines and be used in fitness centers and in home gyms to make sure users are correctly doing exercise routines. This would reduce the indicent of performing the wrong form and possibly reduce the number of injuries due to bad exercise form.
